=== Verwendetetes Betriebssystem ===
(X)ubuntu 18.04.01 LTS

=== Starten der Pipeline ===

1. In Linux-Terminal zu root wechseln: sudo su - [Für Zugang zu mariadb]

2. Im Terminal zum Verzeichnis mit den Rohdaten ((1)-(3) herunterzuladen von der Softwarearchitektur-Homepage) und zum Awk-Skript commands.awk (4) wechseln. Das Verzeichnis muss also enthalten:

(1) 2018-austria-latest.osm [riesige OSM-Datei mit allen Daten für Österreich]
(2) Graz.poly [Grenzen von Graz]
(3) graz_poi_complete.csv [Sehenswürdigkeiten von Graz]
(4) commands.awk [das von mir erstellte Awk-Skript]

3. Dort Start der Command-Pipeline: 

osmfilter 2018-austria-latest.osm --keep= --keep-nodes="shop=" | osmconvert - -B=Graz.poly --all-to-nodes --csv="@id @lon @lat name shop website" | grep -v -w 'vacant' | sed 's/\"/\\"/g' | awk -f commands.awk | mariadb

Ergebnis: MariaDB-Datenbank "shop_finder_db" mit Tabellen "shops" und "points of interest"


===Zwischenergebnisse  zum Testen/Anschauen (mit Ausgabeumleitung in Dateien bzw. Pipe zur Daten)===

osmfilter 2018-austria-latest.osm --keep= --keep-nodes="shop=" > shops_in_austria.osm 
osmconvert shops_in_austria.osm -B=Graz.poly --all-to-nodes --csv="@id @lon @lat name shop website" > shops-in_graz.csv
cat shops_in_graz.csv | grep -v -w 'vacant' | sed 's/\"/\\"/g' | awk -f commands.awk > sql_output.log

=== Datenbereinigung ===

* Herausgefiltert werden, entsprechend der Aufgabenstellung von Task1, die Geschäfte ohne Namen.
* Außerdem alle leerstehenden Geschäftslokale mit "vacant" als Kategorie. (Zum Teil haben die auch noch Namen in der OpenStreetMap-Datei, etwa "Schlecker", dazu "vacant" als Kategorie und werden deshalb nicht schon als namenlose Geschäfte herausgefiltert.)
* Manchmal steht in der OSM-Datei nur "yes" statt einem Eintrag, z.B. beim der Homepage. Die "yes" werden im Awk-Skript gelöscht, außer "yes" steht beim Namen - für den Fall, dass jemand auf die Idee kommen sollte ein Geschäft "yes" zu nennen ;-)


=== Aktuelle Funktionalität, eventuelle Erweiterungen/Abänderungen ===

* Bis jetzt liest die Pipeline die Shop-ID, Höhe, Länge, Namen, Kategorie und Website von der OpenStreeMap-Datei ein und macht daraus eine Tabelle shops mit den entsprechenden Attributen und der ID als Primärschlüssel. Zusätzlich wird aus graz_poi_complete.csv eine Tabelle points_of_interest gebildet (siehe commands.awk) .

* Es wäre zu überlegen, ob wir im Sinne einer sauberen Abbildung in ein Datenbankshema nicht eine eigene Tabelle mit den Shop-Kategorien per Awk-Skript erstellen und sie mit einem (künstlichen) Fremdschlüssel mit der shops-Tabelle vernknüpfen. Wär ein bisschen tricky, aber ich hab programmiertechnisch schon eine Idee, wie man das in der Pipeline bzw. mit Awk-Skript angehen könnte.